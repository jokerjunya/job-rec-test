#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆã—ã¦ã€å®Œå…¨ãªæ±‚äººæƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import json
from pathlib import Path


class DatasetIntegrator:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.job_recommendation_df: Optional[pd.DataFrame] = None
        self.linkedin_jobs_df: Optional[pd.DataFrame] = None
        self.salary_df: Optional[pd.DataFrame] = None
        self.integrated_df: Optional[pd.DataFrame] = None
    
    def load_job_recommendation(self, filepath: str) -> pd.DataFrame:
        """
        ç¾åœ¨ã®Job Recommendation Datasetã‚’èª­ã¿è¾¼ã‚€
        """
        print(f"ğŸ“– Loading Job Recommendation Dataset from {filepath}...")
        df = pd.read_csv(filepath)
        
        # ã‚¹ã‚­ãƒ«ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        df['User_Skills_List'] = df['User_Skills'].str.split(', ')
        df['Job_Requirements_List'] = df['Job_Requirements'].str.split(', ')
        
        print(f"   âœ… Loaded {len(df)} rows")
        print(f"   ğŸ“Š Unique Users: {df['User_ID'].nunique()}")
        print(f"   ğŸ“Š Unique Jobs: {df['Job_ID'].nunique()}")
        
        self.job_recommendation_df = df
        return df
    
    def load_linkedin_jobs(self, filepath: str) -> pd.DataFrame:
        """
        LinkedIn Job Postings Datasetã‚’èª­ã¿è¾¼ã‚€
        """
        print(f"ğŸ“– Loading LinkedIn Job Postings from {filepath}...")
        df = pd.read_csv(filepath)
        
        # job_idã‚’æ¨™æº–åŒ–ï¼ˆå°æ–‡å­—/å¤§æ–‡å­—ã®çµ±ä¸€ï¼‰
        if 'job_id' in df.columns:
            df['job_id'] = df['job_id'].astype(int)
        elif 'Job_ID' in df.columns:
            df['job_id'] = df['Job_ID'].astype(int)
            df = df.rename(columns={'Job_ID': 'job_id'})
        
        print(f"   âœ… Loaded {len(df)} rows")
        print(f"   ğŸ“Š Unique Jobs: {df['job_id'].nunique()}")
        
        self.linkedin_jobs_df = df
        return df
    
    def load_salary_data(self, filepath: str) -> pd.DataFrame:
        """
        Data Science Job Salaries Datasetã‚’èª­ã¿è¾¼ã‚€
        """
        print(f"ğŸ“– Loading Salary Data from {filepath}...")
        df = pd.read_csv(filepath)
        
        # job_titleã‚’æ¨™æº–åŒ–
        if 'job_title' in df.columns:
            df['job_title_normalized'] = df['job_title'].str.lower().str.strip()
        
        print(f"   âœ… Loaded {len(df)} rows")
        
        self.salary_df = df
        return df
    
    def normalize_job_requirements(self, requirements: str) -> List[str]:
        """
        ã‚¹ã‚­ãƒ«è¦ä»¶ã‚’æ­£è¦åŒ–ã—ã¦ãƒªã‚¹ãƒˆã«å¤‰æ›
        """
        if pd.isna(requirements):
            return []
        return [skill.strip() for skill in str(requirements).split(',')]
    
    def match_jobs_by_skills(self, tolerance: float = 0.7) -> pd.DataFrame:
        """
        ã‚¹ã‚­ãƒ«è¦ä»¶ã‚’ä½¿ã£ã¦Job_IDã¨job_idã‚’ãƒãƒƒãƒãƒ³ã‚°
        """
        if self.job_recommendation_df is None or self.linkedin_jobs_df is None:
            raise ValueError("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print("\nğŸ” Matching jobs by skill requirements...")
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªJob_IDã¨Job_Requirementsã®çµ„ã¿åˆã‚ã›ã‚’å–å¾—
        job_requirements = self.job_recommendation_df[
            ['Job_ID', 'Job_Requirements']
        ].drop_duplicates()
        
        # ã‚¹ã‚­ãƒ«ãƒãƒƒãƒãƒ³ã‚°ç”¨ã®è¾æ›¸ã‚’ä½œæˆ
        job_id_mapping = {}
        
        for _, row in job_requirements.iterrows():
            job_id = row['Job_ID']
            requirements = set(self.normalize_job_requirements(row['Job_Requirements']))
            
            # LinkedIn Datasetã¨ãƒãƒƒãƒãƒ³ã‚°
            for _, linkedin_row in self.linkedin_jobs_df.iterrows():
                linkedin_id = linkedin_row['job_id']
                
                # job_requirementsã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‹ç¢ºèª
                if 'job_requirements' in linkedin_row:
                    linkedin_reqs = set(
                        self.normalize_job_requirements(linkedin_row['job_requirements'])
                    )
                elif 'requirements' in linkedin_row:
                    linkedin_reqs = set(
                        self.normalize_job_requirements(linkedin_row['requirements'])
                    )
                else:
                    continue
                
                # ã‚¹ã‚­ãƒ«ã®ä¸€è‡´åº¦ã‚’è¨ˆç®—
                if len(requirements) > 0 and len(linkedin_reqs) > 0:
                    intersection = requirements.intersection(linkedin_reqs)
                    union = requirements.union(linkedin_reqs)
                    similarity = len(intersection) / len(union) if len(union) > 0 else 0
                    
                    if similarity >= tolerance:
                        if job_id not in job_id_mapping:
                            job_id_mapping[job_id] = []
                        job_id_mapping[job_id].append({
                            'linkedin_id': linkedin_id,
                            'similarity': similarity
                        })
        
        print(f"   âœ… Matched {len(job_id_mapping)} jobs")
        return job_id_mapping
    
    def integrate_datasets(
        self,
        use_skill_matching: bool = True,
        fallback_to_id_match: bool = True
    ) -> pd.DataFrame:
        """
        3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ
        """
        print("\nğŸ”— Integrating datasets...")
        
        if self.job_recommendation_df is None:
            raise ValueError("Job Recommendation DatasetãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Job Recommendation Datasetã‚’ãƒ™ãƒ¼ã‚¹ã«
        integrated = self.job_recommendation_df.copy()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: LinkedIn Datasetã¨çµåˆï¼ˆJob_IDã§ç›´æ¥ãƒãƒƒãƒï¼‰
        if self.linkedin_jobs_df is not None:
            print("   ğŸ“ Merging with LinkedIn Job Postings...")
            
            if use_skill_matching:
                # ã‚¹ã‚­ãƒ«ãƒãƒƒãƒãƒ³ã‚°ã‚’ä½¿ç”¨
                job_mapping = self.match_jobs_by_skills()
                # æœ€è‰¯ã®ãƒãƒƒãƒã‚’é¸æŠ
                mapping_df = pd.DataFrame([
                    {
                        'Job_ID': job_id,
                        'linkedin_job_id': matches[0]['linkedin_id'],
                        'skill_similarity': matches[0]['similarity']
                    }
                    for job_id, matches in job_mapping.items()
                    if len(matches) > 0
                ])
                
                # LinkedInãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                linkedin_merged = mapping_df.merge(
                    self.linkedin_jobs_df,
                    left_on='linkedin_job_id',
                    right_on='job_id',
                    how='left',
                    suffixes=('', '_linkedin')
                )
                
                integrated = integrated.merge(
                    linkedin_merged[['Job_ID'] + [
                        col for col in linkedin_merged.columns 
                        if col not in ['Job_ID', 'linkedin_job_id']
                    ]],
                    on='Job_ID',
                    how='left'
                )
            elif fallback_to_id_match:
                # Job_IDã§ç›´æ¥ãƒãƒƒãƒ
                integrated = integrated.merge(
                    self.linkedin_jobs_df,
                    left_on='Job_ID',
                    right_on='job_id',
                    how='left',
                    suffixes=('', '_linkedin')
                )
            
            print(f"   âœ… Merged LinkedIn data: {integrated['job_title'].notna().sum()} jobs matched")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: Salary Datasetã¨çµåˆï¼ˆjob_titleã§ãƒãƒƒãƒï¼‰
        if self.salary_df is not None and 'job_title' in integrated.columns:
            print("   ğŸ’° Merging with Salary Data...")
            
            # job_titleã‚’æ­£è¦åŒ–
            integrated['job_title_normalized'] = integrated['job_title'].str.lower().str.strip()
            
            # çµ¦ä¸ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ï¼ˆåŒã˜è·ç¨®ã®å¹³å‡çµ¦ä¸ã‚’è¨ˆç®—ï¼‰
            salary_agg = self.salary_df.groupby('job_title_normalized').agg({
                'salary_in_usd': ['mean', 'median', 'min', 'max', 'count']
            }).reset_index()
            
            salary_agg.columns = [
                'job_title_normalized',
                'avg_salary_usd',
                'median_salary_usd',
                'min_salary_usd',
                'max_salary_usd',
                'salary_data_count'
            ]
            
            integrated = integrated.merge(
                salary_agg,
                on='job_title_normalized',
                how='left'
            )
            
            print(f"   âœ… Merged salary data: {integrated['avg_salary_usd'].notna().sum()} jobs matched")
        
        self.integrated_df = integrated
        print(f"\nâœ… Integration complete! Total rows: {len(integrated)}")
        
        return integrated
    
    def save_integrated_dataset(self, output_path: str, format: str = 'csv'):
        """
        çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜
        """
        if self.integrated_df is None:
            raise ValueError("çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«integrate_datasets()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        print(f"\nğŸ’¾ Saving integrated dataset to {output_path}...")
        
        if format == 'csv':
            self.integrated_df.to_csv(output_path, index=False)
        elif format == 'parquet':
            self.integrated_df.to_parquet(output_path, index=False)
        elif format == 'json':
            self.integrated_df.to_json(output_path, orient='records', indent=2)
        
        print(f"   âœ… Saved successfully!")
    
    def generate_mapping_report(self, output_path: str):
        """
        ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        """
        if self.integrated_df is None:
            raise ValueError("çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        
        report = {
            'total_rows': len(self.integrated_df),
            'unique_users': self.integrated_df['User_ID'].nunique() if 'User_ID' in self.integrated_df else 0,
            'unique_jobs': self.integrated_df['Job_ID'].nunique() if 'Job_ID' in self.integrated_df else 0,
            'linkedin_matched': self.integrated_df['job_title'].notna().sum() if 'job_title' in self.integrated_df else 0,
            'salary_matched': self.integrated_df['avg_salary_usd'].notna().sum() if 'avg_salary_usd' in self.integrated_df else 0,
            'columns': list(self.integrated_df.columns)
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š Mapping report saved to {output_path}")
        return report


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("=" * 60)
    print("ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    integrator = DatasetIntegrator()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
    # æ³¨: å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„
    try:
        integrator.load_job_recommendation('Job Datsset.csv')
    except FileNotFoundError:
        print("âš ï¸  Job Recommendation DatasetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # LinkedIn Datasetï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰
    linkedin_path = input("\nğŸ“ LinkedIn Job Postings CSVã®ãƒ‘ã‚¹ (Enterã§ã‚¹ã‚­ãƒƒãƒ—): ").strip()
    if linkedin_path and Path(linkedin_path).exists():
        integrator.load_linkedin_jobs(linkedin_path)
    
    # Salary Datasetï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰
    salary_path = input("ğŸ“ Salary Dataset CSVã®ãƒ‘ã‚¹ (Enterã§ã‚¹ã‚­ãƒƒãƒ—): ").strip()
    if salary_path and Path(salary_path).exists():
        integrator.load_salary_data(salary_path)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ
    if integrator.job_recommendation_df is not None:
        integrated = integrator.integrate_datasets(
            use_skill_matching=True,
            fallback_to_id_match=True
        )
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        output_file = 'integrated_job_dataset.csv'
        integrator.save_integrated_dataset(output_file)
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        integrator.generate_mapping_report('integration_report.json')
        
        print("\n" + "=" * 60)
        print("âœ… çµ±åˆå®Œäº†!")
        print("=" * 60)
    else:
        print("\nâŒ çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == '__main__':
    main()

